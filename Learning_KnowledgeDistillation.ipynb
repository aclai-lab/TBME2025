{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pygad\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder = 'ResidualProducts'\n",
    "\n",
    "df = pd.read_csv(f'{result_folder}/ID-Features-Vote.csv')\n",
    "\n",
    "# Define the bin thresholds\n",
    "bins = [\n",
    "    {\"name\": \"bin1\", \"thresholds\": [(0, 25), (26, 50)]},\n",
    "    {\"name\": \"bin2\", \"thresholds\": [(0, 21), (32, 50)]},\n",
    "    {\"name\": \"bin3\", \"thresholds\": [(0, 16), (37, 50)]},\n",
    "    {\"name\": \"bin4\", \"thresholds\": [(0, 11), (42, 50)]}\n",
    "]\n",
    "\n",
    "# Initialize a list to store metrics for all bins\n",
    "metrics_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each bin\n",
    "for bin_info in bins:\n",
    "    bin_name = bin_info[\"name\"]\n",
    "    thresholds = bin_info[\"thresholds\"]\n",
    "    \n",
    "    # Binarize the target variable based on thresholds\n",
    "    bin_data = df.copy()\n",
    "    bin_data['vote'] = np.where(\n",
    "        (bin_data['vote'] >= thresholds[0][0]) & (bin_data['vote'] <= thresholds[0][1]), 0,\n",
    "        np.where(\n",
    "            (bin_data['vote'] >= thresholds[1][0]) & (bin_data['vote'] <= thresholds[1][1]), 1, np.nan\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Drop rows where the target is NaN (outside the defined bins)\n",
    "    bin_data = bin_data.dropna(subset=['vote'])\n",
    "    \n",
    "    # Remove unnecessary columns\n",
    "    bin_data = bin_data.drop(columns=['ID', 'PAINTING'])\n",
    "    \n",
    "    # Separate features (X) and target (y)\n",
    "    X = bin_data.drop(columns=['vote'])\n",
    "    y = bin_data['vote']\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Define the fitness function for the genetic algorithm\n",
    "    def fitness_function(ga_instance, solution, solution_idx):\n",
    "        selected_features = np.where(solution == 1)[0]\n",
    "        if len(selected_features) == 0:\n",
    "            return 0  # Avoid empty selections\n",
    "        \n",
    "        X_train_selected = X_train.iloc[:, selected_features]\n",
    "        X_test_selected = X_test.iloc[:, selected_features]\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        predictions = model.predict(X_test_selected)\n",
    "        return accuracy_score(y_test, predictions)\n",
    "    \n",
    "    # Set up the genetic algorithm\n",
    "    num_generations = 100\n",
    "    num_parents_mating = 5\n",
    "    sol_per_pop = 20\n",
    "    num_genes = X_train.shape[1]\n",
    "    \n",
    "    ga_instance = pygad.GA(\n",
    "        num_generations=num_generations,\n",
    "        num_parents_mating=num_parents_mating,\n",
    "        fitness_func=fitness_function,\n",
    "        sol_per_pop=sol_per_pop,\n",
    "        num_genes=num_genes,\n",
    "        gene_space=[0, 1]\n",
    "    )\n",
    "    \n",
    "    # Run the genetic algorithm\n",
    "    ga_instance.run()\n",
    "    \n",
    "    # Save the fitness convergence plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ga_instance.plot_fitness(title=f\"Fitness Convergence for {bin_name}\")\n",
    "    plt.savefig(f\"{result_folder}/{bin_name}_fitness_convergence.png\")  # Save the plot to a file\n",
    "    plt.close()  # Close the plot to free up memory\n",
    "    \n",
    "    # Get the best solution (selected features)\n",
    "    solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "    selected_features_indices = np.where(solution == 1)[0]\n",
    "    selected_features = X.columns[selected_features_indices]\n",
    "    \n",
    "    # Convert selected features to a comma-separated string\n",
    "    selected_features_str = \", \".join(selected_features)\n",
    "    \n",
    "    # Train the final model using the selected features\n",
    "    X_train_selected = X_train.iloc[:, selected_features_indices]\n",
    "    X_test_selected = X_test.iloc[:, selected_features_indices]\n",
    "    \n",
    "    final_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    final_model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Save the final model to a file\n",
    "    with open(f'{result_folder}/{bin_name}_random_forest_model.pkl', 'wb') as file:\n",
    "        pickle.dump(final_model, file)\n",
    "    \n",
    "    # Evaluate the final model\n",
    "    y_pred = final_model.predict(X_test_selected)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Extract values from the confusion matrix\n",
    "    TN, FP, FN, TP = conf_matrix.ravel()\n",
    "    \n",
    "    # Calculate Sensitivity, Specificity, PPV, and NPV\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    ppv = TP / (TP + FP)\n",
    "    npv = TN / (TN + FN)\n",
    "    \n",
    "    # Store metrics in a dictionary\n",
    "    metrics = {\n",
    "        \"bin_number\": bin_name,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"sensitivity\": sensitivity,\n",
    "        \"specificity\": specificity,\n",
    "        \"ppv\": ppv,\n",
    "        \"npv\": npv,\n",
    "        \"selected_features\": selected_features_str  # Add selected features as a string\n",
    "    }\n",
    "    \n",
    "    # Append metrics to the list\n",
    "    metrics_list.append(metrics)\n",
    "    \n",
    "    # Print metrics for the current bin\n",
    "    print(f\"Metrics for {bin_name}:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Sensitivity: {sensitivity}\")\n",
    "    print(f\"Specificity: {specificity}\")\n",
    "    print(f\"PPV: {ppv}\")\n",
    "    print(f\"NPV: {npv}\")\n",
    "    print(\"Selected Features:\", selected_features_str)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.to_csv(f'{result_folder}/bin_metrics.csv', index=False)\n",
    "\n",
    "print(f\"Metrics saved to '{result_folder}/bin_metrics.csv'.\")\n",
    "print(\"Fitness convergence plots saved for each bin.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focusing on [0,11]->0 [42,50]->1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{result_folder}/ID-Features-Vote.csv')\n",
    "\n",
    "# Define the bin thresholds for the specific bin\n",
    "bin_thresholds = [(0, 11), (42, 50)]\n",
    "\n",
    "# Initialize a list to store metrics for all iterations\n",
    "metrics_list = []\n",
    "\n",
    "# Number of iterations\n",
    "num_iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the bin for each iteration\n",
    "for iteration in range(num_iterations):\n",
    "    # Binarize the target variable based on thresholds\n",
    "    bin_data = df.copy()\n",
    "    bin_data['vote'] = np.where(\n",
    "        (bin_data['vote'] >= bin_thresholds[0][0]) & (bin_data['vote'] <= bin_thresholds[0][1]), 0,\n",
    "        np.where(\n",
    "            (bin_data['vote'] >= bin_thresholds[1][0]) & (bin_data['vote'] <= bin_thresholds[1][1]), 1, np.nan\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Drop rows where the target is NaN (outside the defined bins)\n",
    "    bin_data = bin_data.dropna(subset=['vote'])\n",
    "    \n",
    "    # Remove unnecessary columns\n",
    "    bin_data = bin_data.drop(columns=['ID', 'PAINTING'])\n",
    "    \n",
    "    # Separate features (X) and target (y)\n",
    "    X = bin_data.drop(columns=['vote'])\n",
    "    y = bin_data['vote']\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42 + iteration)  # Change random state for each iteration\n",
    "    \n",
    "    # Define the fitness function for the genetic algorithm\n",
    "    def fitness_function(ga_instance, solution, solution_idx):\n",
    "        selected_features = np.where(solution == 1)[0]\n",
    "        if len(selected_features) == 0:\n",
    "            return 0  # Avoid empty selections\n",
    "        \n",
    "        X_train_selected = X_train.iloc[:, selected_features]\n",
    "        X_test_selected = X_test.iloc[:, selected_features]\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        predictions = model.predict(X_test_selected)\n",
    "        return accuracy_score(y_test, predictions)\n",
    "    \n",
    "    # Set up the genetic algorithm\n",
    "    num_generations = 100\n",
    "    num_parents_mating = 5\n",
    "    sol_per_pop = 20\n",
    "    num_genes = X_train.shape[1]\n",
    "    \n",
    "    ga_instance = pygad.GA(\n",
    "        num_generations=num_generations,\n",
    "        num_parents_mating=num_parents_mating,\n",
    "        fitness_func=fitness_function,\n",
    "        sol_per_pop=sol_per_pop,\n",
    "        num_genes=num_genes,\n",
    "        gene_space=[0, 1]\n",
    "    )\n",
    "    \n",
    "    # Run the genetic algorithm\n",
    "    ga_instance.run()\n",
    "    \n",
    "    # Get the best solution (selected features)\n",
    "    solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "    selected_features_indices = np.where(solution == 1)[0]\n",
    "    selected_features = X.columns[selected_features_indices]\n",
    "    \n",
    "    # Convert selected features to a comma-separated string\n",
    "    selected_features_str = \", \".join(selected_features)\n",
    "    \n",
    "    # Train the final model using the selected features\n",
    "    X_train_selected = X_train.iloc[:, selected_features_indices]\n",
    "    X_test_selected = X_test.iloc[:, selected_features_indices]\n",
    "    \n",
    "    final_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    final_model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Save the final model to a file\n",
    "    with open(f'{result_folder}/random_forest_model_iteration_{iteration + 1}.pkl', 'wb') as file:\n",
    "        pickle.dump(final_model, file)\n",
    "    \n",
    "    # Evaluate the final model\n",
    "    y_pred = final_model.predict(X_test_selected)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Extract values from the confusion matrix\n",
    "    TN, FP, FN, TP = conf_matrix.ravel()\n",
    "    \n",
    "    # Calculate Sensitivity, Specificity, PPV, and NPV\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    ppv = TP / (TP + FP)\n",
    "    npv = TN / (TN + FN)\n",
    "    \n",
    "    # Store metrics in a dictionary\n",
    "    metrics = {\n",
    "        \"iteration\": iteration + 1,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"sensitivity\": sensitivity,\n",
    "        \"specificity\": specificity,\n",
    "        \"ppv\": ppv,\n",
    "        \"npv\": npv,\n",
    "        \"selected_features\": selected_features_str,  # Add selected features as a string\n",
    "    }\n",
    "    \n",
    "    # Append metrics to the list\n",
    "    metrics_list.append(metrics)\n",
    "    \n",
    "    # Print metrics for the current iteration\n",
    "    print(f\"Metrics for iteration {iteration + 1}:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Sensitivity: {sensitivity}\")\n",
    "    print(f\"Specificity: {specificity}\")\n",
    "    print(f\"PPV: {ppv}\")\n",
    "    print(f\"NPV: {npv}\")\n",
    "    print(\"Selected Features:\", selected_features_str)\n",
    "    print(\"Model saved\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.to_csv(f'{result_folder}/iteration_metrics.csv', index=False)\n",
    "\n",
    "print(f\"Metrics saved to '{result_folder}/iteration_metrics.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge Distillation via Mimicking Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original dataset\n",
    "df = pd.read_csv(f'{result_folder}/ID-Features-Vote.csv')\n",
    "\n",
    "# Load the trained Random Forest model\n",
    "with open(f'{result_folder}/random_forest_model_iteration_9.pkl', 'rb') as file:\n",
    "    rf_model = pickle.load(file)\n",
    "\n",
    "# Select the features used by the Random Forest model \n",
    "# (assuming they are the same as those used during training)\n",
    "# If the selected features were saved separately, they should be loaded from a file or a variable\n",
    "selected_features = [\n",
    "    'min_fixations_per_area',  # Minimum number of fixations per area\n",
    "    'max_fixations_per_area',  # Maximum number of fixations per area\n",
    "    'mean_fixations_per_area',  # Mean number of fixations per area\n",
    "    'last_series_num_fixations',  # Number of fixations in the last series\n",
    "    'last_is_longest_duration',  # Whether the last fixation series has the longest duration\n",
    "    'total_duration_fixations',  # Total duration of fixations\n",
    "    'min_dilatation',  # Minimum pupil dilation\n",
    "    'max_dilatation',  # Maximum pupil dilation\n",
    "    'mean_dilatation',  # Mean pupil dilation\n",
    "    'std_dilatation',  # Standard deviation of pupil dilation\n",
    "    'instance_area_mean_dilatation_min',  # Minimum mean pupil dilation in an instance area\n",
    "    'instance_area_mean_dilatation_std',  # Standard deviation of mean pupil dilation in an instance area\n",
    "    'is_max_dilatation_in_first_instance_area',  # Whether the maximum pupil dilation occurs in the first instance area\n",
    "    'returns_to_initial_area',  # Number of times the gaze returns to the initial area\n",
    "    'initial_instance_area_duration_ms',  # Duration of the first instance area in milliseconds\n",
    "    'initial_instance_area_n_fixations'  # Number of fixations in the first instance area\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over different maximum depths for the Decision Tree (ranging from 3 to 5)\n",
    "for depth in range(3, 6):  \n",
    "\n",
    "    # Step 1: Prepare the dataset\n",
    "\n",
    "    # Extract the selected features from the dataset\n",
    "    X = df[selected_features]  \n",
    "\n",
    "    # Use the pre-trained Random Forest model to predict labels (0 or 1) for each instance in the dataset\n",
    "    y_pred = rf_model.predict(X)  \n",
    "\n",
    "    # Append the predicted labels as a new column in the dataset\n",
    "    df['RFLabel'] = y_pred  \n",
    "\n",
    "    # Save the updated dataset, now including the predicted labels, to a CSV file\n",
    "    df.to_csv(f'{result_folder}/RFLabeled_ID-Features-Vote.csv', index=False)  \n",
    "    print(f\"Labeled dataset saved in '{result_folder}/RFLabeled_ID-Features-Vote.csv'.\")\n",
    "\n",
    "    # Step 2: Train a Decision Tree to mimic the Random Forest model\n",
    "\n",
    "    # Define the feature matrix (X_train) and target labels (y_train) for training the Decision Tree\n",
    "    X_train = df[selected_features]  \n",
    "    y_train = df['RFLabel']  \n",
    "\n",
    "    # Create a Decision Tree classifier with a specific maximum depth\n",
    "    # The depth is controlled by the loop variable `depth`, allowing experimentation with different tree depths\n",
    "    dt_model = DecisionTreeClassifier(random_state=42, max_depth=depth)  \n",
    "\n",
    "    # Train the Decision Tree model on the dataset\n",
    "    dt_model.fit(X_train, y_train)  \n",
    "\n",
    "    # Step 3: Evaluate the Decision Tree’s performance\n",
    "\n",
    "    # Predict labels on the training set to check how well the Decision Tree mimics the Random Forest model\n",
    "    y_pred_dt = dt_model.predict(X_train)  \n",
    "\n",
    "    # Compute the accuracy score to measure how well the Decision Tree replicates the Random Forest’s decisions\n",
    "    accuracy = accuracy_score(y_train, y_pred_dt)  \n",
    "    print(f\"Decision Tree accuracy on the training set (depth={depth}): {accuracy}\")\n",
    "\n",
    "    # Step 4: Save the trained Decision Tree model\n",
    "\n",
    "    # Serialize and save the trained Decision Tree model to a file\n",
    "    with open(f'{result_folder}/dt_mimick_depth{depth}.pkl', 'wb') as file:  \n",
    "        pickle.dump(dt_model, file)  \n",
    "\n",
    "    print(f\"Decision Tree model (depth={depth}) saved in '{result_folder}/dt_mimick_depth{depth}.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tree_model(pickle_file):\n",
    "    \"\"\"\n",
    "    Load a decision tree model from a pickle file.\n",
    "\n",
    "    This function deserializes and loads a trained Decision Tree model\n",
    "    that has been previously saved using Python's `pickle` module.\n",
    "\n",
    "    Args:\n",
    "        pickle_file (str): The path to the pickle file containing the saved model.\n",
    "\n",
    "    Returns:\n",
    "        DecisionTreeClassifier: The deserialized Decision Tree model.\n",
    "    \"\"\"\n",
    "    # Open the specified pickle file in read-binary mode\n",
    "    with open(pickle_file, \"rb\") as f:\n",
    "        # Load the serialized Decision Tree model from the file\n",
    "        tree = pickle.load(f)\n",
    "    \n",
    "    # Return the loaded model to be used for predictions or further analysis\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_rules(tree, feature_names=None):\n",
    "    \"\"\"\n",
    "    Extracts decision rules from a trained Decision Tree model in a human-readable text format.\n",
    "\n",
    "    This function uses `export_text` from `sklearn.tree` to generate a textual representation\n",
    "    of the decision rules learned by the tree. It is useful for understanding how the \n",
    "    Decision Tree makes decisions and for debugging or model interpretation.\n",
    "\n",
    "    Args:\n",
    "        tree (DecisionTreeClassifier or DecisionTreeRegressor): \n",
    "            A trained scikit-learn Decision Tree model.\n",
    "        feature_names (list of str, optional): \n",
    "            A list of feature names corresponding to the input features used in training the tree.\n",
    "            If not provided, default feature indices will be used.\n",
    "\n",
    "    Returns:\n",
    "        str: A textual representation of the tree's decision rules.\n",
    "    \"\"\"\n",
    "    return export_text(tree, feature_names=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rules_cnf(decision_tree, feature_names):\n",
    "    \"\"\"\n",
    "    Extracts the decision rules of a trained Decision Tree model in Conjunctive Normal Form (CNF) \n",
    "    along with the corresponding decision path.\n",
    "\n",
    "    This function traverses the Decision Tree recursively, extracting the rules that define \n",
    "    each decision path. It constructs logical conditions in CNF, which makes it useful \n",
    "    for interpretability and logical reasoning.\n",
    "\n",
    "    Args:\n",
    "        decision_tree (DecisionTreeClassifier or DecisionTreeRegressor): \n",
    "            A trained scikit-learn Decision Tree model.\n",
    "        feature_names (list of str): \n",
    "            A list of feature names corresponding to the input features used in training the tree.\n",
    "\n",
    "    Returns:\n",
    "        dict: A nested dictionary where each leaf node contains:\n",
    "              - `\"class\"`: The predicted class (for classification trees) or regression value.\n",
    "              - `\"rule\"`: The corresponding CNF-style rule that leads to that prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Access the internal tree structure of the trained Decision Tree model\n",
    "    tree_ = decision_tree.tree_\n",
    "\n",
    "    def recurse(node, path):\n",
    "        \"\"\"\n",
    "        Recursively traverses the Decision Tree to construct CNF rules.\n",
    "\n",
    "        Args:\n",
    "            node (int): The current node index in the Decision Tree structure.\n",
    "            path (list of str): A list of conditions accumulated along the path.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the decision rule and class at each leaf node.\n",
    "        \"\"\"\n",
    "        if tree_.feature[node] != -2:  # Internal node\n",
    "            # Extract the feature name and threshold used for splitting at this node\n",
    "            name = feature_names[tree_.feature[node]]\n",
    "            threshold = tree_.threshold[node]\n",
    "\n",
    "            # Construct the left and right paths based on the threshold condition\n",
    "            left_path = path + [f\"{name} <= {threshold}\"]\n",
    "            right_path = path + [f\"{name} > {threshold}\"]\n",
    "\n",
    "            # Recursively process the left and right child nodes\n",
    "            return {\n",
    "                \"left\": recurse(tree_.children_left[node], left_path),\n",
    "                \"right\": recurse(tree_.children_right[node], right_path)\n",
    "            }\n",
    "        else:  # Leaf node\n",
    "            # Extract the class prediction at this leaf (the class with the highest probability)\n",
    "            return {\"class\": int(np.argmax(tree_.value[node])), \"rule\": \" AND \".join(path)}\n",
    "\n",
    "    # Start recursion from the root node (index 0)\n",
    "    return recurse(0, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rules_on_dataset(tree, dataset, feature_names):\n",
    "    \"\"\"\n",
    "    Evaluates the support and confidence of decision rules extracted from a Decision Tree \n",
    "    on a given dataset.\n",
    "\n",
    "    This function:\n",
    "    1. Extracts the CNF-style rules from a trained Decision Tree.\n",
    "    2. Matches each rule against the dataset to determine how frequently it applies (support).\n",
    "    3. Computes the confidence of each rule by measuring how often it correctly predicts the class.\n",
    "\n",
    "    Args:\n",
    "        tree (DecisionTreeClassifier): \n",
    "            A trained scikit-learn Decision Tree model.\n",
    "        dataset (pd.DataFrame): \n",
    "            A Pandas DataFrame containing the feature values and actual class labels.\n",
    "            - The last column is assumed to contain the true class labels.\n",
    "        feature_names (list of str): \n",
    "            A list of feature names corresponding to the input features used in training the tree.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing:\n",
    "            - `\"rule\"`: The logical condition defining the rule.\n",
    "            - `\"class\"`: The predicted class according to the rule.\n",
    "            - `\"support\"`: The proportion of instances in the dataset that satisfy the rule.\n",
    "            - `\"confidence\"`: The proportion of instances satisfying the rule that also match the predicted class.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract decision rules in CNF format from the Decision Tree\n",
    "    rules = extract_rules_cnf(tree, feature_names)\n",
    "    \n",
    "    # Initialize a list to store the evaluation results\n",
    "    results = []\n",
    "    \n",
    "    # Total number of samples in the dataset (for support calculation)\n",
    "    total_samples = len(dataset)\n",
    "\n",
    "    def match_rule(row, rule):\n",
    "        \"\"\"\n",
    "        Checks whether a given dataset row satisfies a specific decision rule.\n",
    "\n",
    "        Args:\n",
    "            row (pd.Series): A single instance from the dataset.\n",
    "            rule (dict): A rule dictionary containing a 'rule' key with logical conditions.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the row satisfies the rule, False otherwise.\n",
    "        \"\"\"\n",
    "        if \"rule\" not in rule:\n",
    "            return False  # If no rule is present, no match is possible\n",
    "        \n",
    "        # Split the rule into its individual conditions\n",
    "        conditions = rule[\"rule\"].split(\" AND \")\n",
    "        \n",
    "        # Evaluate each condition on the dataset row\n",
    "        for condition in conditions:\n",
    "            feature, op, threshold = condition.split()  # Parse condition format: \"Feature_X <= 2.5\"\n",
    "            threshold = float(threshold)  # Convert threshold to float\n",
    "            \n",
    "            # Check if the condition holds for the current row\n",
    "            if op == \"<=\" and row[feature] > threshold:\n",
    "                return False\n",
    "            if op == \">\" and row[feature] <= threshold:\n",
    "                return False\n",
    "        \n",
    "        return True  # If all conditions are satisfied, return True\n",
    "\n",
    "    def traverse_rules(node):\n",
    "        \"\"\"\n",
    "        Recursively traverses the extracted decision tree rules and evaluates them on the dataset.\n",
    "\n",
    "        Args:\n",
    "            node (dict): A rule node from the extracted CNF rules.\n",
    "        \"\"\"\n",
    "        if \"class\" in node:  # If the node is a leaf (i.e., a final decision rule)\n",
    "            # Apply the rule to all rows in the dataset to determine how many match\n",
    "            matching_rows = dataset.apply(lambda row: match_rule(row, node), axis=1)\n",
    "\n",
    "            # Compute support: proportion of dataset instances satisfying the rule\n",
    "            support = matching_rows.sum() / total_samples\n",
    "\n",
    "            # Compute confidence: proportion of matching instances that also have the expected class\n",
    "            if matching_rows.sum() > 0:\n",
    "                confidence = (matching_rows & (dataset.iloc[:, -1] == node[\"class\"]).values).sum() / matching_rows.sum()\n",
    "            else:\n",
    "                confidence = 0  # Avoid division by zero when no rows match\n",
    "\n",
    "            # Store the rule evaluation results\n",
    "            results.append({\n",
    "                \"rule\": node[\"rule\"], \n",
    "                \"class\": node[\"class\"], \n",
    "                \"support\": support, \n",
    "                \"confidence\": confidence\n",
    "            })\n",
    "        else:\n",
    "            # Recursively evaluate rules in left and right child nodes\n",
    "            traverse_rules(node[\"left\"])\n",
    "            traverse_rules(node[\"right\"])\n",
    "\n",
    "    # Start traversing from the root of the rules dictionary\n",
    "    traverse_rules(rules)\n",
    "\n",
    "    # Return the results as a Pandas DataFrame\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over different depths of the trained Decision Tree models\n",
    "for d in range(3, 6):\n",
    "    # Construct the file path for the serialized Decision Tree model corresponding to the current depth\n",
    "    pickle_file = f'{result_folder}/dt_mimick_depth{d}.pkl'  # Replace with actual file path\n",
    "\n",
    "    # Define the dataset file containing features and predicted labels from the Random Forest model\n",
    "    dataset_file = f'{result_folder}/RFLabeled_ID-Features-Vote.csv'\n",
    "\n",
    "    # Load the dataset from the CSV file\n",
    "    dataset = pd.read_csv(dataset_file)\n",
    "\n",
    "    # Define the list of feature names (must match those used during Decision Tree training)\n",
    "    feature_names = [\n",
    "        'min_fixations_per_area', 'max_fixations_per_area', 'mean_fixations_per_area',\n",
    "        'last_series_num_fixations', 'last_is_longest_duration', 'total_duration_fixations',\n",
    "        'min_dilatation', 'max_dilatation', 'mean_dilatation', 'std_dilatation',\n",
    "        'instance_area_mean_dilatation_min', 'instance_area_mean_dilatation_std',\n",
    "        'is_max_dilatation_in_first_instance_area', 'returns_to_initial_area',\n",
    "        'initial_instance_area_duration_ms', 'initial_instance_area_n_fixations'\n",
    "    ]  # Replace with actual feature names if needed\n",
    "\n",
    "    # Load the trained Decision Tree model from the corresponding pickle file\n",
    "    tree = load_tree_model(pickle_file)\n",
    "\n",
    "    # Evaluate the extracted decision rules on the dataset\n",
    "    results = evaluate_rules_on_dataset(tree, dataset, feature_names)\n",
    "\n",
    "    # Save the extracted rules and their evaluation metrics to a CSV file\n",
    "    output_file = f\"{result_folder}/rules_depth{d}.csv\"\n",
    "    results.to_csv(output_file, index=False)\n",
    "\n",
    "    # Print confirmation message\n",
    "    print(f\"Rules saved in {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
