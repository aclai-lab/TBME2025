{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pygad\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder = 'ResidualProducts'\n",
    "\n",
    "df = pd.read_csv(f'{result_folder}/ID-Features-Vote.csv')\n",
    "\n",
    "# Define the bin thresholds\n",
    "bins = [\n",
    "    {\"name\": \"bin1\", \"thresholds\": [(0, 25), (26, 50)]},\n",
    "    {\"name\": \"bin2\", \"thresholds\": [(0, 21), (32, 50)]},\n",
    "    {\"name\": \"bin3\", \"thresholds\": [(0, 16), (37, 50)]},\n",
    "    {\"name\": \"bin4\", \"thresholds\": [(0, 11), (42, 50)]}\n",
    "]\n",
    "\n",
    "# Initialize a list to store metrics for all bins\n",
    "metrics_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each bin\n",
    "for bin_info in bins:\n",
    "    bin_name = bin_info[\"name\"]\n",
    "    thresholds = bin_info[\"thresholds\"]\n",
    "    \n",
    "    # Binarize the target variable based on thresholds\n",
    "    bin_data = df.copy()\n",
    "    bin_data['vote'] = np.where(\n",
    "        (bin_data['vote'] >= thresholds[0][0]) & (bin_data['vote'] <= thresholds[0][1]), 0,\n",
    "        np.where(\n",
    "            (bin_data['vote'] >= thresholds[1][0]) & (bin_data['vote'] <= thresholds[1][1]), 1, np.nan\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Drop rows where the target is NaN (outside the defined bins)\n",
    "    bin_data = bin_data.dropna(subset=['vote'])\n",
    "    \n",
    "    # Remove unnecessary columns\n",
    "    bin_data = bin_data.drop(columns=['ID', 'PAINTING'])\n",
    "    \n",
    "    # Separate features (X) and target (y)\n",
    "    X = bin_data.drop(columns=['vote'])\n",
    "    y = bin_data['vote']\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Define the fitness function for the genetic algorithm\n",
    "    def fitness_function(ga_instance, solution, solution_idx):\n",
    "        selected_features = np.where(solution == 1)[0]\n",
    "        if len(selected_features) == 0:\n",
    "            return 0  # Avoid empty selections\n",
    "        \n",
    "        X_train_selected = X_train.iloc[:, selected_features]\n",
    "        X_test_selected = X_test.iloc[:, selected_features]\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        predictions = model.predict(X_test_selected)\n",
    "        return accuracy_score(y_test, predictions)\n",
    "    \n",
    "    # Set up the genetic algorithm\n",
    "    num_generations = 100\n",
    "    num_parents_mating = 5\n",
    "    sol_per_pop = 20\n",
    "    num_genes = X_train.shape[1]\n",
    "    \n",
    "    ga_instance = pygad.GA(\n",
    "        num_generations=num_generations,\n",
    "        num_parents_mating=num_parents_mating,\n",
    "        fitness_func=fitness_function,\n",
    "        sol_per_pop=sol_per_pop,\n",
    "        num_genes=num_genes,\n",
    "        gene_space=[0, 1]\n",
    "    )\n",
    "    \n",
    "    # Run the genetic algorithm\n",
    "    ga_instance.run()\n",
    "    \n",
    "    # Save the fitness convergence plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ga_instance.plot_fitness(title=f\"Fitness Convergence for {bin_name}\")\n",
    "    plt.savefig(f\"{result_folder}/{bin_name}_fitness_convergence.png\")  # Save the plot to a file\n",
    "    plt.close()  # Close the plot to free up memory\n",
    "    \n",
    "    # Get the best solution (selected features)\n",
    "    solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "    selected_features_indices = np.where(solution == 1)[0]\n",
    "    selected_features = X.columns[selected_features_indices]\n",
    "    \n",
    "    # Convert selected features to a comma-separated string\n",
    "    selected_features_str = \", \".join(selected_features)\n",
    "    \n",
    "    # Train the final model using the selected features\n",
    "    X_train_selected = X_train.iloc[:, selected_features_indices]\n",
    "    X_test_selected = X_test.iloc[:, selected_features_indices]\n",
    "    \n",
    "    final_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    final_model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Save the final model to a file\n",
    "    with open(f'{result_folder}/{bin_name}_random_forest_model.pkl', 'wb') as file:\n",
    "        pickle.dump(final_model, file)\n",
    "    \n",
    "    # Evaluate the final model\n",
    "    y_pred = final_model.predict(X_test_selected)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Extract values from the confusion matrix\n",
    "    TN, FP, FN, TP = conf_matrix.ravel()\n",
    "    \n",
    "    # Calculate Sensitivity, Specificity, PPV, and NPV\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    ppv = TP / (TP + FP)\n",
    "    npv = TN / (TN + FN)\n",
    "    \n",
    "    # Store metrics in a dictionary\n",
    "    metrics = {\n",
    "        \"bin_number\": bin_name,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"sensitivity\": sensitivity,\n",
    "        \"specificity\": specificity,\n",
    "        \"ppv\": ppv,\n",
    "        \"npv\": npv,\n",
    "        \"selected_features\": selected_features_str  # Add selected features as a string\n",
    "    }\n",
    "    \n",
    "    # Append metrics to the list\n",
    "    metrics_list.append(metrics)\n",
    "    \n",
    "    # Print metrics for the current bin\n",
    "    print(f\"Metrics for {bin_name}:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Sensitivity: {sensitivity}\")\n",
    "    print(f\"Specificity: {specificity}\")\n",
    "    print(f\"PPV: {ppv}\")\n",
    "    print(f\"NPV: {npv}\")\n",
    "    print(\"Selected Features:\", selected_features_str)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.to_csv(f'{result_folder}/bin_metrics.csv', index=False)\n",
    "\n",
    "print(f\"Metrics saved to '{result_folder}/bin_metrics.csv'.\")\n",
    "print(\"Fitness convergence plots saved for each bin.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focusing on [0,11]->0 [42,50]->1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{result_folder}/ID-Features-Vote.csv')\n",
    "\n",
    "# Define the bin thresholds for the specific bin\n",
    "bin_thresholds = [(0, 11), (42, 50)]\n",
    "\n",
    "# Initialize a list to store metrics for all iterations\n",
    "metrics_list = []\n",
    "\n",
    "# Number of iterations\n",
    "num_iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the bin for each iteration\n",
    "for iteration in range(num_iterations):\n",
    "    # Binarize the target variable based on thresholds\n",
    "    bin_data = df.copy()\n",
    "    bin_data['vote'] = np.where(\n",
    "        (bin_data['vote'] >= bin_thresholds[0][0]) & (bin_data['vote'] <= bin_thresholds[0][1]), 0,\n",
    "        np.where(\n",
    "            (bin_data['vote'] >= bin_thresholds[1][0]) & (bin_data['vote'] <= bin_thresholds[1][1]), 1, np.nan\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Drop rows where the target is NaN (outside the defined bins)\n",
    "    bin_data = bin_data.dropna(subset=['vote'])\n",
    "    \n",
    "    # Remove unnecessary columns\n",
    "    bin_data = bin_data.drop(columns=['ID', 'PAINTING'])\n",
    "    \n",
    "    # Separate features (X) and target (y)\n",
    "    X = bin_data.drop(columns=['vote'])\n",
    "    y = bin_data['vote']\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42 + iteration)  # Change random state for each iteration\n",
    "    \n",
    "    # Define the fitness function for the genetic algorithm\n",
    "    def fitness_function(ga_instance, solution, solution_idx):\n",
    "        selected_features = np.where(solution == 1)[0]\n",
    "        if len(selected_features) == 0:\n",
    "            return 0  # Avoid empty selections\n",
    "        \n",
    "        X_train_selected = X_train.iloc[:, selected_features]\n",
    "        X_test_selected = X_test.iloc[:, selected_features]\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        predictions = model.predict(X_test_selected)\n",
    "        return accuracy_score(y_test, predictions)\n",
    "    \n",
    "    # Set up the genetic algorithm\n",
    "    num_generations = 100\n",
    "    num_parents_mating = 5\n",
    "    sol_per_pop = 20\n",
    "    num_genes = X_train.shape[1]\n",
    "    \n",
    "    ga_instance = pygad.GA(\n",
    "        num_generations=num_generations,\n",
    "        num_parents_mating=num_parents_mating,\n",
    "        fitness_func=fitness_function,\n",
    "        sol_per_pop=sol_per_pop,\n",
    "        num_genes=num_genes,\n",
    "        gene_space=[0, 1]\n",
    "    )\n",
    "    \n",
    "    # Run the genetic algorithm\n",
    "    ga_instance.run()\n",
    "    \n",
    "    # Get the best solution (selected features)\n",
    "    solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "    selected_features_indices = np.where(solution == 1)[0]\n",
    "    selected_features = X.columns[selected_features_indices]\n",
    "    \n",
    "    # Convert selected features to a comma-separated string\n",
    "    selected_features_str = \", \".join(selected_features)\n",
    "    \n",
    "    # Train the final model using the selected features\n",
    "    X_train_selected = X_train.iloc[:, selected_features_indices]\n",
    "    X_test_selected = X_test.iloc[:, selected_features_indices]\n",
    "    \n",
    "    final_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    final_model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Save the final model to a file\n",
    "    with open(f'{result_folder}/random_forest_model_iteration_{iteration + 1}.pkl', 'wb') as file:\n",
    "        pickle.dump(final_model, file)\n",
    "    \n",
    "    # Evaluate the final model\n",
    "    y_pred = final_model.predict(X_test_selected)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Extract values from the confusion matrix\n",
    "    TN, FP, FN, TP = conf_matrix.ravel()\n",
    "    \n",
    "    # Calculate Sensitivity, Specificity, PPV, and NPV\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    ppv = TP / (TP + FP)\n",
    "    npv = TN / (TN + FN)\n",
    "    \n",
    "    # Store metrics in a dictionary\n",
    "    metrics = {\n",
    "        \"iteration\": iteration + 1,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"sensitivity\": sensitivity,\n",
    "        \"specificity\": specificity,\n",
    "        \"ppv\": ppv,\n",
    "        \"npv\": npv,\n",
    "        \"selected_features\": selected_features_str,  # Add selected features as a string\n",
    "    }\n",
    "    \n",
    "    # Append metrics to the list\n",
    "    metrics_list.append(metrics)\n",
    "    \n",
    "    # Print metrics for the current iteration\n",
    "    print(f\"Metrics for iteration {iteration + 1}:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Sensitivity: {sensitivity}\")\n",
    "    print(f\"Specificity: {specificity}\")\n",
    "    print(f\"PPV: {ppv}\")\n",
    "    print(f\"NPV: {npv}\")\n",
    "    print(\"Selected Features:\", selected_features_str)\n",
    "    print(\"Model saved\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all metrics to a CSV file\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.to_csv(f'{result_folder}/iteration_metrics.csv', index=False)\n",
    "\n",
    "print(f\"Metrics saved to '{result_folder}/iteration_metrics.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge Distillation via Mimic Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il dataset originale\n",
    "df = pd.read_csv(f'{result_folder}/ID-Features-Vote.csv')\n",
    "\n",
    "# Carica il modello Random Forest addestrato\n",
    "with open(f'{result_folder}/random_forest_model_iteration_9.pkl', 'rb') as file:\n",
    "    rf_model = pickle.load(file)\n",
    "\n",
    "# Seleziona le features utilizzate dalla RF (supponiamo che siano le stesse del modello salvato)\n",
    "# Se hai salvato le features selezionate, puoi caricarle da un file o da una variabile\n",
    "selected_features = ['min_fixations_per_area',\n",
    "'max_fixations_per_area',\n",
    "'mean_fixations_per_area',\n",
    "'last_series_num_fixations',\n",
    "'last_is_longest_duration',\n",
    "'total_duration_fixations',\n",
    "'min_dilatation',\n",
    "'max_dilatation',\n",
    "'mean_dilatation',\n",
    "'std_dilatation',\n",
    "'instance_area_mean_dilatation_min',\n",
    "'instance_area_mean_dilatation_std',\n",
    "'is_max_dilatation_in_first_instance_area',\n",
    "'returns_to_initial_area',\n",
    "'initial_instance_area_duration_ms',\n",
    "'initial_instance_area_n_fixations']  # Sostituisci con le features effettive selezionate dalla RF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crea il dataset con le features selezionate\n",
    "X = df[selected_features]\n",
    "\n",
    "# Usa la RF per predire le etichette (0 o 1)\n",
    "y_pred = rf_model.predict(X)\n",
    "\n",
    "# Aggiungi le etichette predette al dataset originale\n",
    "df['RFLabel'] = y_pred\n",
    "\n",
    "# Salva il nuovo dataset etichettato\n",
    "df.to_csv(f'{result_folder}/RFLabeled_ID-Features-Vote.csv', index=False)\n",
    "\n",
    "print(f\"Dataset etichettato salvato in '{result_folder}/RFLabeled_ID-Features-Vote.csv'.\")\n",
    "\n",
    "# Ora addestriamo un albero decisionale per mimare la RF\n",
    "# Separiamo le features e il target (le etichette predette dalla RF)\n",
    "X_train = df[selected_features]\n",
    "y_train = df['RFLabel']\n",
    "\n",
    "# Crea e addestra l'albero decisionale\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Valuta l'albero decisionale (opzionale, per verificare quanto bene mima la RF)\n",
    "y_pred_dt = dt_model.predict(X_train)\n",
    "accuracy = accuracy_score(y_train, y_pred_dt)\n",
    "print(f\"Accuracy dell'albero decisionale sul training set: {accuracy}\")\n",
    "\n",
    "# Save the final model to a file\n",
    "with open(f'{result_folder}/decision_tree_mimic.pkl', 'wb') as file:\n",
    "    pickle.dump(dt_model, file)\n",
    "\n",
    "print(f\"Albero decisionale salvato in '{result_folder}/decision_tree_mimic.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
